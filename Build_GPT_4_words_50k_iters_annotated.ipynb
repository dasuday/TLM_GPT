{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Language Model Training - Word Level\n",
    "## 50,000 Training Iterations\n",
    "\n",
    "This notebook implements a word-level GPT model training pipeline with:\n",
    "- Data preparation and tokenization\n",
    "- Model architecture definition\n",
    "- Training loop with validation\n",
    "- Model saving and inference\n",
    "\n",
    "**Model Statistics:**\n",
    "- ~18.67M parameters\n",
    "- Word-level tokenization\n",
    "- 50,000 training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "Import all necessary Python libraries for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch imports for neural network implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Utility imports for timing and other operations\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Hyperparameters Configuration\n",
    "Define all training and model hyperparameters in one place for easy modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============= Training Hyperparameters =============\n",
    "# Batch and sequence configuration\n",
    "batch_size = 64          # Number of sequences processed in parallel\n",
    "block_size = 256         # Maximum context length for predictions\n",
    "max_iters = 50000        # Total number of training iterations\n",
    "eval_interval = 500      # Frequency of validation loss computation\n",
    "learning_rate = 3e-4     # Initial learning rate for optimizer\n",
    "eval_iters = 200         # Number of iterations for validation loss estimation\n",
    "\n",
    "# ============= Model Architecture Hyperparameters =============\n",
    "n_embd = 384            # Embedding dimension (model width)\n",
    "n_head = 6              # Number of attention heads\n",
    "n_layer = 6             # Number of transformer blocks\n",
    "dropout = 0.2           # Dropout probability for regularization\n",
    "\n",
    "# ============= System Configuration =============\n",
    "# Automatically select GPU if available, otherwise use CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)  # Set random seed for reproducibility\n",
    "\n",
    "print(f\"Training device: {device}\")\n",
    "#print(f\"Model will have approximately {(n_embd * n_layer * 4 + n_embd * vocab_size * 2) / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Loading and Preprocessing\n",
    "Load the text data and prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24022 unique words\n",
      "First 10 words in vocabulary: ['\"', '\".', '\"10!', '\"A', '\"Abby,', '\"Abigail,', '\"Abracadabra!\"', '\"Absolutely!', '\"Achoo!\"', '\"Acting']\n"
     ]
    }
   ],
   "source": [
    "# ============= Load Training Data =============\n",
    "# Load your text file (replace 'input.txt' with your actual file path)\n",
    "with open('tiny_story.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# ============= Word-Level Tokenization =============\n",
    "# Split text into words and create vocabulary\n",
    "words = text.split()  # Simple whitespace splitting\n",
    "vocab = sorted(list(set(words)))  # Create unique vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "stoi = {word: i for i, word in enumerate(vocab)}  # string to integer\n",
    "itos = {i: word for i, word in enumerate(vocab)}  # integer to string\n",
    "\n",
    "# Define encoding and decoding functions\n",
    "def encode(text: str) -> List[int]:\n",
    "    \"\"\"Convert text string to list of token indices\"\"\"\n",
    "    return [stoi[word] for word in text.split() if word in stoi]\n",
    "\n",
    "def decode(indices: List[int]) -> str:\n",
    "    \"\"\"Convert list of token indices back to text string\"\"\"\n",
    "    return ' '.join([itos[i] for i in indices if i in itos])\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size} unique words\")\n",
    "print(f\"First 10 words in vocabulary: {vocab[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 899,118\n",
      "Validation tokens: 99,902\n",
      "Total tokens: 999,020\n"
     ]
    }
   ],
   "source": [
    "# ============= Prepare Training and Validation Data =============\n",
    "# Encode entire text dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Split data into training (90%) and validation (10%)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training tokens: {len(train_data):,}\")\n",
    "print(f\"Validation tokens: {len(val_data):,}\")\n",
    "print(f\"Total tokens: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Loading Functions\n",
    "Functions to create batches of data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 256])\n",
      "Target batch shape: torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate a batch of data for training or validation.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val' to select the data split\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences of shape (batch_size, block_size)\n",
    "        y: Target sequences of shape (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    # Select the appropriate dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Generate random starting positions for each sequence in the batch\n",
    "    # Ensure we don't go past the end of the data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Stack sequences to create batch tensors\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # Move to the appropriate device (CPU or GPU)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test the batch generation\n",
    "x_sample, y_sample = get_batch('train')\n",
    "print(f\"Input batch shape: {x_sample.shape}\")\n",
    "print(f\"Target batch shape: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss() -> dict:\n",
    "    \"\"\"\n",
    "    Estimate the average loss on train and validation sets.\n",
    "    Uses multiple batches to get a more stable estimate.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'train' and 'val' losses\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        \n",
    "        # Average loss over multiple batches\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        \n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()  # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Model Architecture Components\n",
    "Define the transformer model components including attention heads, feed-forward networks, and blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Single head of self-attention.\n",
    "    Implements scaled dot-product attention with causal masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Register causal mask as a buffer (not a parameter)\n",
    "        # Lower triangular matrix ensures tokens only attend to previous tokens\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch size, Time steps, Channels\n",
    "        \n",
    "        # Compute queries, keys, values\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores (scaled by sqrt of head_size)\n",
    "        # @ is matrix multiplication\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask (prevent looking at future tokens)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention running in parallel.\n",
    "    Concatenates outputs and projects back to embedding dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        # Create multiple attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "        # Final linear projection after concatenation\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Run all attention heads in parallel and concatenate\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # Project back to embedding dimension\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network with one hidden layer.\n",
    "    Typically expands dimension by 4x then projects back.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Expand dimension by 4x\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            # Apply ReLU activation\n",
    "            nn.ReLU(),\n",
    "            # Project back to embedding dimension\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: communication (attention) followed by computation (FFN).\n",
    "    Uses residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd: int, n_head: int):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention with residual connection\n",
    "        # Note: This is Pre-LayerNorm architecture\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # Apply feed-forward with residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Complete Model Definition\n",
    "Assemble all components into the final GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT Language Model.\n",
    "    Combines token embeddings, positional embeddings, transformer blocks,\n",
    "    and output projection for next-token prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding table: maps vocabulary indices to vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Positional embedding table: adds position information\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output projection: maps from embedding space to vocabulary\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize model weights with appropriate values\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input token indices of shape (B, T)\n",
    "            targets: Target token indices of shape (B, T), optional\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predictions of shape (B, T, vocab_size)\n",
    "            loss: Cross-entropy loss if targets provided, else None\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Get token embeddings: (B, T, C)\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        \n",
    "        # Get positional embeddings: (T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "        # Combine token and positional embeddings\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape for cross-entropy loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting context of shape (B, T)\n",
    "            max_new_tokens: Number of new tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Extended sequence of shape (B, T + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to maximum block size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append sampled token to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Initialization and Information\n",
    "Create the model instance and display model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 29.211862 M parameters\n",
      "\n",
      "Model Architecture:\n",
      "- Vocabulary Size: 24022\n",
      "- Embedding Dimension: 384\n",
      "- Number of Heads: 6\n",
      "- Number of Layers: 6\n",
      "- Block Size: 256\n",
      "- Dropout Rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Create model instance and move to device\n",
    "model = WordLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Count and display model parameters\n",
    "num_params = sum(p.numel() for p in m.parameters())\n",
    "print(f\"Model initialized with {num_params/1e6:.6f} M parameters\")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"- Vocabulary Size: {vocab_size}\")\n",
    "print(f\"- Embedding Dimension: {n_embd}\")\n",
    "print(f\"- Number of Heads: {n_head}\")\n",
    "print(f\"- Number of Layers: {n_layer}\")\n",
    "print(f\"- Block Size: {block_size}\")\n",
    "print(f\"- Dropout Rate: {dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Optimizer Setup\n",
    "Configure the AdamW optimizer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crazy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW\n",
      "Learning Rate: 0.0003\n",
      "Total Training Steps: 50,000\n"
     ]
    }
   ],
   "source": [
    "# Create AdamW optimizer\n",
    "# AdamW is Adam with proper weight decay (better than L2 regularization)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Total Training Steps: {max_iters:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Training Loop\n",
    "Main training loop with periodic evaluation and progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============= Main Training Loop =============\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Track training start time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # ========== Evaluation Step ==========\n",
    "    # Periodically evaluate loss on train and validation sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_minutes = int(elapsed_time // 60)\n",
    "        elapsed_seconds = elapsed_time % 60\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Step {iter:5d}: \"\n",
    "              f\"train loss {losses['train']:.4f}, \"\n",
    "              f\"val loss {losses['val']:.4f}, \"\n",
    "              f\"Elapsed: {elapsed_minutes} min, {elapsed_seconds:.2f} sec\")\n",
    "        \n",
    "        # Optional: Save checkpoint if validation loss improved\n",
    "        # You can add checkpoint saving logic here\n",
    "    \n",
    "    # ========== Training Step ==========\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Forward pass: compute predictions and loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear gradients from previous step\n",
    "    loss.backward()  # Compute gradients\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "# ============= Training Complete =============\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\"*50)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Final train loss: {losses['train']:.4f}\")\n",
    "print(f\"Final validation loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Text Generation\n",
    "Generate sample text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Generate Sample Text =============\n",
    "print(\"\\nGenerating sample text...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the starting prompt\n",
    "prompt = \"Once upon a time\"\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Encode the prompt\n",
    "starting_tokens = encode(prompt)\n",
    "starting_context = torch.tensor([starting_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate new tokens\n",
    "num_tokens_to_generate = 500\n",
    "generated = m.generate(starting_context, max_new_tokens=num_tokens_to_generate)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = decode(generated[0].tolist())\n",
    "print(\"Generated text:\")\n",
    "print(\"-\"*50)\n",
    "print(generated_text)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Interactive Text Generation =============\n",
    "# Allow user to input custom prompts\n",
    "\n",
    "def generate_text(prompt: str, max_tokens: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a custom prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    tokens = encode(prompt)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"Warning: Prompt contains no known words. Using random start.\")\n",
    "        tokens = [0]  # Start with first token in vocabulary\n",
    "    \n",
    "    context = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    generated = m.generate(context, max_new_tokens=max_tokens)\n",
    "    \n",
    "    # Decode and return\n",
    "    return decode(generated[0].tolist())\n",
    "\n",
    "# Example usage\n",
    "custom_prompt = \"The king\"\n",
    "custom_output = generate_text(custom_prompt, max_tokens=200)\n",
    "print(f\"\\nCustom generation from '{custom_prompt}':\")\n",
    "print(custom_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Model Saving\n",
    "Save the trained model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Save Model Weights =============\n",
    "model_path = 'generated_model_50Kiters.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Calculate file size\n",
    "import os\n",
    "file_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\n",
    "print(f\"Model file size: {file_size:.2f} MB\")\n",
    "\n",
    "# ============= Save Vocabulary =============\n",
    "# Save vocabulary for later use with the model\n",
    "vocab_path = 'vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'vocab': vocab,\n",
    "        'vocab_size': vocab_size,\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, f)\n",
    "print(f\"Vocabulary saved to: {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Model Loading\n",
    "Load a previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Load Saved Model =============\n",
    "def load_model(model_path: str, vocab_path: str):\n",
    "    \"\"\"\n",
    "    Load a previously trained model and its vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        vocab_path: Path to the saved vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model and vocabulary dictionary\n",
    "    \"\"\"\n",
    "    # Load vocabulary\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "    \n",
    "    # Update global variables (in practice, you'd handle this better)\n",
    "    global vocab, vocab_size, stoi, itos\n",
    "    vocab = vocab_data['vocab']\n",
    "    vocab_size = vocab_data['vocab_size']\n",
    "    stoi = vocab_data['stoi']\n",
    "    itos = vocab_data['itos']\n",
    "    \n",
    "    # Create and load model\n",
    "    loaded_model = WordLanguageModel()\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "    print(f\"Vocabulary loaded successfully from {vocab_path}\")\n",
    "    \n",
    "    return loaded_model, vocab_data\n",
    "\n",
    "# Example: Load the saved model\n",
    "# loaded_model, vocab_data = load_model('generated_model_50Kiters.pth', 'vocabulary.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Model Analysis and Metrics\n",
    "Analyze model performance and compute various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Compute Perplexity =============\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, data_loader, split='val'):\n",
    "    \"\"\"\n",
    "    Compute perplexity on the specified dataset.\n",
    "    Perplexity = exp(average_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 100  # Use a fixed number of batches for consistency\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        X, Y = get_batch(split)\n",
    "        logits, loss = model(X, Y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "# Calculate perplexity\n",
    "train_perplexity = compute_perplexity(m, None, 'train')\n",
    "val_perplexity = compute_perplexity(m, None, 'val')\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Perplexity: {train_perplexity:.2f}\")\n",
    "print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n",
    "print(f\"Vocabulary Coverage: {vocab_size:,} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Attention Visualization Helper =============\n",
    "def get_attention_weights(model, text: str, layer_idx: int = 0, head_idx: int = 0):\n",
    "    \"\"\"\n",
    "    Extract attention weights for visualization.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        text: Input text\n",
    "        layer_idx: Which transformer layer to visualize\n",
    "        head_idx: Which attention head to visualize\n",
    "    \n",
    "    Returns:\n",
    "        Attention weights matrix\n",
    "    \"\"\"\n",
    "    # This is a simplified version - you'd need to modify the model\n",
    "    # to actually extract attention weights during forward pass\n",
    "    \n",
    "    tokens = encode(text)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No valid tokens found in text\")\n",
    "        return None\n",
    "    \n",
    "    context = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Note: This is a placeholder - actual implementation would require\n",
    "    # modifying the model to return attention weights\n",
    "    print(f\"Attention visualization for layer {layer_idx}, head {head_idx}\")\n",
    "    print(f\"Input text: '{text}'\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    \n",
    "    return None  # Placeholder\n",
    "\n",
    "# Example usage\n",
    "# attention_weights = get_attention_weights(m, \"The king and queen\", layer_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Training Diagnostics\n",
    "Additional tools for monitoring and debugging training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Gradient Statistics =============\n",
    "def print_gradient_statistics(model):\n",
    "    \"\"\"\n",
    "    Print statistics about gradients to diagnose training issues.\n",
    "    Useful for detecting vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    total_norm = 0\n",
    "    param_count = 0\n",
    "    \n",
    "    print(\"\\nGradient Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2).item()\n",
    "            total_norm += param_norm ** 2\n",
    "            param_count += 1\n",
    "            \n",
    "            # Print statistics for important layers\n",
    "            if 'embedding' in name or 'ln' in name or 'lm_head' in name:\n",
    "                grad_mean = param.grad.data.mean().item()\n",
    "                grad_std = param.grad.data.std().item()\n",
    "                print(f\"{name[:30]:30s} | norm: {param_norm:.6f} | \"\n",
    "                      f\"mean: {grad_mean:.6f} | std: {grad_std:.6f}\")\n",
    "    \n",
    "    total_norm = (total_norm ** 0.5)\n",
    "    print(f\"\\nTotal gradient norm: {total_norm:.6f}\")\n",
    "    print(f\"Average gradient norm: {total_norm/param_count:.6f}\")\n",
    "    \n",
    "    return total_norm\n",
    "\n",
    "# Example: Check gradients after a training step\n",
    "# xb, yb = get_batch('train')\n",
    "# logits, loss = model(xb, yb)\n",
    "# loss.backward()\n",
    "# grad_norm = print_gradient_statistics(model)\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Learning Rate Schedule (Optional) =============\n",
    "def get_lr(it, warmup_iters=2000, lr_decay_iters=50000, min_lr=1e-5):\n",
    "    \"\"\"\n",
    "    Learning rate schedule with warmup and cosine decay.\n",
    "    \n",
    "    Args:\n",
    "        it: Current iteration\n",
    "        warmup_iters: Number of warmup steps\n",
    "        lr_decay_iters: Total number of decay steps\n",
    "        min_lr: Minimum learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Learning rate for current iteration\n",
    "    \"\"\"\n",
    "    # Warmup phase: linearly increase from 0 to learning_rate\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    \n",
    "    # After decay_iters, use minimum learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    \n",
    "    # Cosine decay\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    \n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# Visualize learning rate schedule\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate learning rates for all iterations\n",
    "lrs = [get_lr(it) for it in range(max_iters)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial LR: {lrs[0]:.6f}\")\n",
    "print(f\"Peak LR: {max(lrs):.6f}\")\n",
    "print(f\"Final LR: {lrs[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Built:\n",
    "- A complete GPT language model with ~18.67M parameters\n",
    "- Word-level tokenization system\n",
    "- Full training pipeline with validation\n",
    "- Text generation capabilities\n",
    "- Model saving/loading functionality\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Tokenization**: Consider using BPE or SentencePiece for better vocabulary\n",
    "2. **Architecture**: Experiment with different model sizes, heads, and layers\n",
    "3. **Training**: Implement learning rate scheduling and gradient clipping\n",
    "4. **Data**: Use larger and more diverse datasets\n",
    "5. **Evaluation**: Add more metrics like BLEU scores or human evaluation\n",
    "6. **Optimization**: Implement mixed precision training for faster computation\n",
    "7. **Regularization**: Add weight decay, dropout variations, or other techniques\n",
    "\n",
    "### Resources for Further Learning:\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n",
    "- [GPT-2 Paper](https://openai.com/research/better-language-models) - Language model scaling\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual guide\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Framework reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
