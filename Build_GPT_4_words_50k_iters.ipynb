{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) - 899118\n",
      "len(val_data) - 99902\n",
      "18.668502 M parameters\n",
      "step 0: train loss 10.2289, val loss 10.2324, Elapsed time: 10 minutes, 2.496495 seconds\n",
      "step 500: train loss 5.1260, val loss 5.0997, Elapsed time: 52 minutes, 33.499671 seconds\n",
      "step 1000: train loss 4.5442, val loss 4.5369, Elapsed time: 94 minutes, 0.815617 seconds\n",
      "step 1500: train loss 4.2436, val loss 4.2746, Elapsed time: 134 minutes, 56.385277 seconds\n",
      "step 2000: train loss 3.9941, val loss 4.0474, Elapsed time: 175 minutes, 47.994271 seconds\n",
      "step 2500: train loss 3.7801, val loss 3.8854, Elapsed time: 216 minutes, 37.825264 seconds\n",
      "step 3000: train loss 3.5954, val loss 3.7689, Elapsed time: 257 minutes, 32.351256 seconds\n",
      "step 3500: train loss 3.4501, val loss 3.6754, Elapsed time: 298 minutes, 26.279417 seconds\n",
      "step 4000: train loss 3.3069, val loss 3.6055, Elapsed time: 339 minutes, 18.404215 seconds\n",
      "step 4500: train loss 3.1853, val loss 3.5469, Elapsed time: 380 minutes, 5.470889 seconds\n",
      "step 5000: train loss 3.0798, val loss 3.5068, Elapsed time: 421 minutes, 3.054040 seconds\n",
      "step 5500: train loss 2.9733, val loss 3.4706, Elapsed time: 461 minutes, 56.611129 seconds\n",
      "step 6000: train loss 2.8724, val loss 3.4467, Elapsed time: 502 minutes, 51.900583 seconds\n",
      "step 6500: train loss 2.7832, val loss 3.4258, Elapsed time: 543 minutes, 36.449418 seconds\n",
      "step 7000: train loss 2.6899, val loss 3.4167, Elapsed time: 584 minutes, 33.762891 seconds\n",
      "step 7500: train loss 2.6109, val loss 3.4104, Elapsed time: 625 minutes, 23.011707 seconds\n",
      "step 8000: train loss 2.5336, val loss 3.4033, Elapsed time: 666 minutes, 15.932625 seconds\n",
      "step 8500: train loss 2.4530, val loss 3.4137, Elapsed time: 706 minutes, 57.069839 seconds\n",
      "step 9000: train loss 2.3769, val loss 3.4078, Elapsed time: 747 minutes, 50.950080 seconds\n",
      "step 9500: train loss 2.3072, val loss 3.4033, Elapsed time: 788 minutes, 38.226670 seconds\n",
      "step 10000: train loss 2.2454, val loss 3.4265, Elapsed time: 829 minutes, 28.474963 seconds\n",
      "step 10500: train loss 2.1840, val loss 3.4200, Elapsed time: 870 minutes, 11.756540 seconds\n",
      "step 11000: train loss 2.1197, val loss 3.4262, Elapsed time: 911 minutes, 5.619566 seconds\n",
      "step 11500: train loss 2.0605, val loss 3.4475, Elapsed time: 951 minutes, 46.463282 seconds\n",
      "step 12000: train loss 2.0026, val loss 3.4756, Elapsed time: 992 minutes, 35.915334 seconds\n",
      "step 12500: train loss 1.9521, val loss 3.4826, Elapsed time: 1033 minutes, 18.716444 seconds\n",
      "step 13000: train loss 1.8973, val loss 3.4890, Elapsed time: 1074 minutes, 10.687662 seconds\n",
      "step 13500: train loss 1.8459, val loss 3.5134, Elapsed time: 1114 minutes, 49.362351 seconds\n",
      "step 14000: train loss 1.7985, val loss 3.5340, Elapsed time: 1155 minutes, 30.242027 seconds\n",
      "step 14500: train loss 1.7460, val loss 3.5545, Elapsed time: 1196 minutes, 11.959565 seconds\n",
      "step 15000: train loss 1.7030, val loss 3.5725, Elapsed time: 1237 minutes, 2.615156 seconds\n",
      "step 15500: train loss 1.6598, val loss 3.5933, Elapsed time: 1277 minutes, 38.088366 seconds\n",
      "step 16000: train loss 1.6199, val loss 3.6277, Elapsed time: 1318 minutes, 22.485167 seconds\n",
      "step 16500: train loss 1.5781, val loss 3.6532, Elapsed time: 1359 minutes, 5.808394 seconds\n",
      "step 17000: train loss 1.5395, val loss 3.6599, Elapsed time: 1399 minutes, 57.552263 seconds\n",
      "step 17500: train loss 1.5012, val loss 3.6859, Elapsed time: 1440 minutes, 42.453863 seconds\n",
      "step 18000: train loss 1.4582, val loss 3.7222, Elapsed time: 1481 minutes, 27.597194 seconds\n",
      "step 18500: train loss 1.4301, val loss 3.7491, Elapsed time: 1522 minutes, 17.415100 seconds\n",
      "step 19000: train loss 1.3887, val loss 3.7831, Elapsed time: 1563 minutes, 16.333005 seconds\n",
      "step 19500: train loss 1.3578, val loss 3.8080, Elapsed time: 1604 minutes, 10.371027 seconds\n",
      "step 20000: train loss 1.3189, val loss 3.8398, Elapsed time: 1645 minutes, 5.679966 seconds\n",
      "step 20500: train loss 1.2966, val loss 3.8352, Elapsed time: 1686 minutes, 1.902862 seconds\n",
      "step 21000: train loss 1.2627, val loss 3.8879, Elapsed time: 1726 minutes, 56.032979 seconds\n",
      "step 21500: train loss 1.2244, val loss 3.8985, Elapsed time: 1767 minutes, 51.320926 seconds\n",
      "step 22000: train loss 1.2018, val loss 3.9206, Elapsed time: 1808 minutes, 40.067137 seconds\n",
      "step 22500: train loss 1.1693, val loss 3.9646, Elapsed time: 1849 minutes, 38.756745 seconds\n",
      "step 23000: train loss 1.1504, val loss 3.9890, Elapsed time: 1890 minutes, 34.874631 seconds\n",
      "step 23500: train loss 1.1150, val loss 4.0205, Elapsed time: 1931 minutes, 33.744710 seconds\n",
      "step 24000: train loss 1.0909, val loss 4.0250, Elapsed time: 1972 minutes, 18.899634 seconds\n",
      "step 24500: train loss 1.0743, val loss 4.0773, Elapsed time: 2013 minutes, 14.932255 seconds\n",
      "step 25000: train loss 1.0408, val loss 4.0885, Elapsed time: 2054 minutes, 2.647862 seconds\n",
      "step 25500: train loss 1.0187, val loss 4.1309, Elapsed time: 2094 minutes, 54.113026 seconds\n",
      "step 26000: train loss 0.9972, val loss 4.1491, Elapsed time: 2135 minutes, 34.027875 seconds\n",
      "step 26500: train loss 0.9734, val loss 4.1650, Elapsed time: 2176 minutes, 31.839042 seconds\n",
      "step 27000: train loss 0.9526, val loss 4.2065, Elapsed time: 2217 minutes, 19.528267 seconds\n",
      "step 27500: train loss 0.9358, val loss 4.2363, Elapsed time: 2258 minutes, 11.797390 seconds\n",
      "step 28000: train loss 0.9140, val loss 4.2394, Elapsed time: 2298 minutes, 54.433734 seconds\n",
      "step 28500: train loss 0.8910, val loss 4.2835, Elapsed time: 2339 minutes, 49.273598 seconds\n",
      "step 29000: train loss 0.8722, val loss 4.2959, Elapsed time: 2380 minutes, 26.851543 seconds\n",
      "step 29500: train loss 0.8519, val loss 4.3170, Elapsed time: 2421 minutes, 11.086830 seconds\n",
      "step 30000: train loss 0.8362, val loss 4.3713, Elapsed time: 2461 minutes, 46.753080 seconds\n",
      "step 30500: train loss 0.8146, val loss 4.4001, Elapsed time: 2502 minutes, 36.654915 seconds\n",
      "step 31000: train loss 0.8004, val loss 4.4106, Elapsed time: 2543 minutes, 7.350265 seconds\n",
      "step 31500: train loss 0.7835, val loss 4.4270, Elapsed time: 2583 minutes, 46.011477 seconds\n",
      "step 32000: train loss 0.7701, val loss 4.4604, Elapsed time: 2624 minutes, 23.035175 seconds\n",
      "step 32500: train loss 0.7522, val loss 4.4853, Elapsed time: 2665 minutes, 6.658703 seconds\n",
      "step 33000: train loss 0.7400, val loss 4.5033, Elapsed time: 2705 minutes, 40.078903 seconds\n",
      "step 33500: train loss 0.7194, val loss 4.5216, Elapsed time: 2746 minutes, 21.189545 seconds\n",
      "step 34000: train loss 0.7049, val loss 4.5496, Elapsed time: 2787 minutes, 4.756829 seconds\n",
      "step 34500: train loss 0.6904, val loss 4.5703, Elapsed time: 2827 minutes, 56.167488 seconds\n",
      "step 35000: train loss 0.6762, val loss 4.6020, Elapsed time: 2868 minutes, 40.882721 seconds\n",
      "step 35500: train loss 0.6659, val loss 4.6320, Elapsed time: 2909 minutes, 28.172410 seconds\n",
      "step 36000: train loss 0.6499, val loss 4.6480, Elapsed time: 2950 minutes, 20.463651 seconds\n",
      "step 36500: train loss 0.6370, val loss 4.6722, Elapsed time: 2991 minutes, 16.549780 seconds\n",
      "step 37000: train loss 0.6273, val loss 4.6896, Elapsed time: 3032 minutes, 8.948813 seconds\n",
      "step 37500: train loss 0.6130, val loss 4.7263, Elapsed time: 3072 minutes, 58.725687 seconds\n",
      "step 38000: train loss 0.5995, val loss 4.7451, Elapsed time: 3113 minutes, 54.117170 seconds\n",
      "step 38500: train loss 0.5926, val loss 4.7672, Elapsed time: 3154 minutes, 54.073263 seconds\n",
      "step 39000: train loss 0.5818, val loss 4.7826, Elapsed time: 3195 minutes, 47.498533 seconds\n",
      "step 39500: train loss 0.5703, val loss 4.7957, Elapsed time: 3236 minutes, 35.845689 seconds\n",
      "step 40000: train loss 0.5586, val loss 4.8640, Elapsed time: 3277 minutes, 34.567680 seconds\n",
      "step 40500: train loss 0.5520, val loss 4.8912, Elapsed time: 3318 minutes, 27.871820 seconds\n",
      "step 41000: train loss 0.5369, val loss 4.8837, Elapsed time: 3359 minutes, 23.633837 seconds\n",
      "step 41500: train loss 0.5336, val loss 4.9139, Elapsed time: 3400 minutes, 9.849566 seconds\n",
      "step 42000: train loss 0.5219, val loss 4.9116, Elapsed time: 3441 minutes, 6.659307 seconds\n",
      "step 42500: train loss 0.5123, val loss 4.9520, Elapsed time: 3482 minutes, 1.613666 seconds\n",
      "step 43000: train loss 0.5054, val loss 4.9514, Elapsed time: 3522 minutes, 56.088837 seconds\n",
      "step 43500: train loss 0.4964, val loss 4.9946, Elapsed time: 3563 minutes, 40.088352 seconds\n",
      "step 44000: train loss 0.4903, val loss 5.0122, Elapsed time: 3604 minutes, 37.286811 seconds\n",
      "step 44500: train loss 0.4795, val loss 5.0282, Elapsed time: 3645 minutes, 29.135100 seconds\n",
      "step 45000: train loss 0.4709, val loss 5.0523, Elapsed time: 3686 minutes, 24.104230 seconds\n",
      "step 45500: train loss 0.4643, val loss 5.0588, Elapsed time: 3727 minutes, 8.066216 seconds\n",
      "step 46000: train loss 0.4542, val loss 5.0880, Elapsed time: 3768 minutes, 6.685741 seconds\n",
      "step 46500: train loss 0.4499, val loss 5.0956, Elapsed time: 3808 minutes, 46.567383 seconds\n",
      "step 47000: train loss 0.4407, val loss 5.1098, Elapsed time: 3849 minutes, 37.117003 seconds\n",
      "step 47500: train loss 0.4362, val loss 5.1496, Elapsed time: 3890 minutes, 20.707814 seconds\n",
      "step 48000: train loss 0.4311, val loss 5.1782, Elapsed time: 3931 minutes, 12.389005 seconds\n",
      "step 48500: train loss 0.4252, val loss 5.2223, Elapsed time: 3971 minutes, 45.898041 seconds\n",
      "step 49000: train loss 0.4165, val loss 5.2132, Elapsed time: 4012 minutes, 27.874743 seconds\n",
      "step 49500: train loss 0.4105, val loss 5.2254, Elapsed time: 4053 minutes, 6.974968 seconds\n",
      "step 49999: train loss 0.4035, val loss 5.2534, Elapsed time: 4093 minutes, 53.348070 seconds\n",
      "\" jar slowly and a baby leopard was picking up and sniffing colors. He says, \"You are so fast I wish I was following my son and I got on the hat. I am blowing The baby of tears in the bowl of tears getting off and respect. The family soon realized that the baby leopard was not a terrible storm. They wondered why the family just kept looking at the causing things for the baby. \"Maybe the baby was having an idea. He said, \"Let's find a baby when we get busy and always be careful when you're grown up and make sure we need more safe.\" One day the baby was feeling very worried. He asked, â€œCan I go now?\" he said, \"Where did that the doctor was weak. His dad said, â€œThat is a special place to support you. You can keep it all different while me. When you're finished, we got to the doctor said mommy, \"I will protect it for you.\" He was not happy, but he knew his dad would love the same job. His dad was so proud of him, and Daniel watched as his dad put a disgusting belch! His little eyes was so happy that he was the little boy smiled again! Story end Story Start Once there was a small boy who had a big dream. He liked to take turns to make sure he was when he was brave and adventurous. One day, he saw a small big ice cream covered around. It was hard about dark and filled with the sun. The little girl slowly reached out his pocket and touched it. It was so soft and thick, colourful and warm. He was so excited he couldn't believe it. The little girl slowly picked up the cone and slowly smiled - it was so big, it tasted amazing. She smiled and looked at everything. There was so many different grazing. he almost touched the soft things. He felt so happy and content. He knew that he would be able to surprise them all. But it was too late. He was a mess the key and the one understood why they were planning to admire its beauty and more of the day had been perfect. Story end Story Start Once there was a puppy who liked to adventure by himself. He always looked for things to put in the house. One day, he saw a vendor playing a tall building with lots of things in the house. The family followed the house and saw a campfire. He saw the fire and it was very messy. He pointed to the front of the vehicle and the man swapped it for its front of him. The man got angry and tried to take it out how brought it anyway. When the man opened his eyes was exhausted and he ran away to find the silver treasure. The man was so happy he had found the amazing gum. He thanked the man and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import timeit\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 50000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the text data\n",
    "with open('tiny_story.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a mapping from words to integers\n",
    "stoi = {word: i for i, word in enumerate(vocab)}\n",
    "itos = {i: word for i, word in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[word] for word in s.split()]\n",
    "decode = lambda l: ' '.join([itos[i] for i in l])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"len(train_data) - {len(train_data)}\")\n",
    "print(f\"len(val_data) - {len(val_data)}\")\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class WordLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = WordLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, Elapsed time: {int(minutes)} minutes, {seconds:.6f} seconds\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a girl named Amy. Amy was a very curious three year old. She loved to explore and one day the big, old house near her house. One day, she asked her mum if she could get there. The new door shook it and said \"Come out. Amy was so excited! She opened her bag and there was a lot of fun things inside. Amy wanted to open the door. She opened the door and stepped out. Suddenly, Amy started to feel a tingle She couldn't believe her eyes. To her surprise, there was an old, pink wall. Amy said, \"Let's go home, and the tree has a bright light. We can spread it from a special sign like this full of eyes and magic spell. Amy loved it! She grabbed the blanket up her door and placed it outside in the wall. Liz couldn't believe her eyes when she heard a voice of wind chimes. it sounded like, \"You found a magical butterfly. \"There is the sky tonight. he said. \"It is so cute!\" said Sally. \"Let's go inside our yard. We need to get closer, mom?\" \"Only if we could find a way to stay close So, Amy stood in the hole and looked around. She saw an apple tree. The tree was so happy that she decided to take a little bit of sun in the tree too big. In the tree began to wash the top, put on the end of the ground. Suddenly, the tree started to zip line. Sally was scared, but she was determined to catch it. Just then, a small voice came out of her door standing by Lucy's shoulder. The girl looked up and saw that the card was now strong and stepped off the tree. At first, then the bird spent all morning when she was done, the little girl laughed and said, â€œThat was so much fun! I'm so happy and fun!\" The end. Story end Story Start There was a robot and little boy were cats too. One day, they went to the park together. The man suddenly heard a loud noise and looked up at his name. The bald man was wearing his voice and gave his dog a new shirt to find a nice, soft fur and the man went home. The end. Story end Story Start Once there was a little boy named John. He loved to move and play with his friends. One day, he went out to the park to play golf. They had so much fun playing down and having a fun time. After a while, they ran back home. When it got dark, the friends said it was time to leave. He was so tired that he wanted to get his furry food so he decided to sleep. But his friends were very smart and kept coming along until they reached a big rock. The bush was different. He felt so proud of himself and wanted to rest. He knew\n"
     ]
    }
   ],
   "source": [
    "starting_text = encode(\"Once upon a time\")\n",
    "starting_context = torch.tensor([starting_text], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(starting_context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'generated_model_50Kiters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = WordLanguageModel()\n",
    "model.load_state_dict(torch.load('generated_model_50Kiters.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
